---
layout: post
title:  "Theory of Information"
date:   2016-07-04 12:33:06 +0800
categories: Outline
---
信息论的复习大纲
=

先验概率：是一个事件根据以往的经验和分析得到的概率，不具备当前情况的任何信息。

后验概率：在先验概率的基础上，由结果推出的概率，类似于条件概率。

自信息量：一个事件一定发生所需要的信息量

信息量：确定多种可能中其中一种可能所需要的试探次数的对数

熵：自信息量的期望，用来表示一个事件集中，平均的自信息量的大小

互信息：事件x1出现给事件x2发生提供的信息量

平均互信息：互信息的期望，两个集合之间，一个集合中事件出现后所给出的关于另一个集合中事件出现的信息量的平均值。

条件降低熵：H(X|Y) <= H(X)

链式法则：H(X,Y) = H(X) + H(Y|X)

信息处理不等式：

Y = f(Z)

X -> Z -> Y

H(X|Z) <= H(X|f(Z)) = H(X|Y)

I(X;Z) >= I(X;f(Z)) = I(X;Y)

--------

M = D<sup>N</sup>

R = (N/L)logD

可达速率：对于给定的信源和编码速率R以及任意的ε>0，若存在一种编码方法，使得L>L<sub>0</sub>时，
P<sub>e</sub> < ε，就称R是可达的。

信源编码定理：若R>H(U)，则R是可达的;若R<H(U)，则R是不可达的

Kraft不等式：长度为n1,n2,...,nk的D元异字头码存在的充分必要条件是：∑<sub>k</sub> D<sup>-nk</sup> <= 1

唯一可译码：对有限长的每个源输出序列，相应的码字序列彼此都可无异议地区分开来。

唯一可译码必须满足Kraft不等式。

不等长编码定理：平均长度n < (H(U)/logD) + 1 ; n >= (H(U)/logD)   

异字头码：码中任何一个码字都不是另一个码字的字头

等长编码：对信源输出的信息采用相同长度的码字表示

不等长编码：对信源输出的信息采用不同长度的码字表示

编码方法：（练习）

* Huffman编码:从叶开始，合并，如果是D元码，要求数目除以(D-1)余1，不满足的话补0，尽量优先使用未使用过的结点，可以使方差最小

* LZ编码:先分段，每个字有个编码，每一段有个编码（段的号 + 符号编码）

* 算术编码:需要先计算码长，计算输入信源符号序列所对应的区间，然后再区间中任取一点，以其二进制表示适当截断作为序列的编码结果。

-------

信道容量：改变输入分布，所能达到的平均互信息的最大值

信道容量的计算：

已知转移概率

* 对称信道：log(输出数) + 转移矩阵第一行的熵的和  （输入等概）
* 准对称信道： 转移矩阵的第一行的每一列的第一个值乘以log(这第一个值除以这一列值的均值)（输入等概）
* 离散无记忆模K加性噪声信道：C = logK - H(Z) ; z = x ⊕ y mod K 
* 积信道(独立并行信道):C = C1 + C2 
* 和信道(并信道):2<sup>C</sup> = 2<sup>C<sub>1</sub></sup> + 2<sup>C<sub>2</sub></sup>
* 高斯噪声信道:0.5*log(1+信噪比) 信噪比： S/σ<sup>2</sup>
* 平行可加高斯噪声信道(注水算法):S = B - σ<sup>2</sup>，只计算σ<sup>2</sup>在B以下相差部分与噪声比值的部分。 C = ∑ 0.5*log(1+ S/σ<sup>2</sup>)

------

编码速率：R = L / N

信道编码定理：给定信道容量为C的离散无记忆信道，R < C是可达的。

最大后验译码准则：取后验概率最大的来将信息译码。

最大似然译码准则：（先验概率等概的情况下，与最大后验一致），取似然函数（先验概率）最大的情况来进行译码。
